{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback, TensorBoard\n",
    "from tensorflow.keras.layers import Conv2D, Reshape\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.backend import epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"/Users/zzhang/Documents/Workspace/Dataset/Pet/\"\n",
    "\n",
    "IMAGE_FOLDER = DATASET_FOLDER + \"images/\"\n",
    "XMLS_FOLDER = \"{}{}/*xml\".format(DATASET_FOLDER,\"annotations/xmls\")\n",
    "TRAIN_CSV = DATASET_FOLDER + \"train.csv\"\n",
    "VALIDATION_CSV = DATASET_FOLDER + \"validation.csv\"\n",
    "SPLIT_RATIO = 0.8\n",
    "\n",
    "TENSORBOARD_LOGDIR = \"./tensorboard\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def pet_csv():\n",
    "    if not os.path.exists(DATASET_FOLDER):\n",
    "        print(\"Dataset not found\")\n",
    "        return\n",
    "\n",
    "    class_names = {}\n",
    "    k = 0\n",
    "    output = []\n",
    "    xml_files = glob.glob(XMLS_FOLDER)\n",
    "    print(len(xml_files))\n",
    "    for i, xml_file in enumerate(xml_files):\n",
    "        tree = ET.parse(xml_file)\n",
    "\n",
    "        path = os.path.join(tree.findtext(\"./filename\"))\n",
    "\n",
    "        height = int(tree.findtext(\"./size/height\"))\n",
    "        width = int(tree.findtext(\"./size/width\"))\n",
    "        xmin = int(tree.findtext(\"./object/bndbox/xmin\"))\n",
    "        ymin = int(tree.findtext(\"./object/bndbox/ymin\"))\n",
    "        xmax = int(tree.findtext(\"./object/bndbox/xmax\"))\n",
    "        ymax = int(tree.findtext(\"./object/bndbox/ymax\"))\n",
    "\n",
    "        basename = os.path.basename(path)\n",
    "        basename = os.path.splitext(basename)[0]\n",
    "        class_name = basename[:basename.rfind(\"_\")].lower()\n",
    "        if class_name not in class_names:\n",
    "            class_names[class_name] = k\n",
    "            k += 1\n",
    "\n",
    "        output.append((path, height, width, xmin, ymin, xmax, ymax, class_name, class_names[class_name]))\n",
    "\n",
    "    # preserve percentage of samples for each class (\"stratified\")\n",
    "    output.sort(key=lambda tup : tup[-1])\n",
    "\n",
    "    lengths = []\n",
    "    i = 0\n",
    "    last = 0\n",
    "    for j, row in enumerate(output):\n",
    "        if last == row[-1]:\n",
    "            i += 1\n",
    "        else:\n",
    "            print(\"class {}: {} images\".format(output[j-1][-2], i))\n",
    "            lengths.append(i)\n",
    "            i = 1\n",
    "            last += 1\n",
    "\n",
    "        print(\"class {}: {} images\".format(output[j-1][-2], i))\n",
    "    lengths.append(i)\n",
    "\n",
    "    with open(TRAIN_CSV, \"w\") as train, open(VALIDATION_CSV, \"w\") as validate:\n",
    "        writer = csv.writer(train, delimiter=\",\")\n",
    "        writer2 = csv.writer(validate, delimiter=\",\")\n",
    "\n",
    "        s = 0\n",
    "        for c in lengths:\n",
    "            for i in range(c):\n",
    "                print(\"{}/{}\".format(s + 1, sum(lengths)), end=\"\\r\")\n",
    "\n",
    "                path, height, width, xmin, ymin, xmax, ymax, class_name, class_id = output[s]\n",
    "\n",
    "                if xmin >= xmax or ymin >= ymax or xmax > width or ymax > height or xmin < 0 or ymin < 0:\n",
    "                    print(\"Warning: {} contains invalid box. Skipped...\".format(path))\n",
    "                    continue\n",
    "\n",
    "                row = [path, height, width, xmin, ymin, xmax, ymax, class_name, class_names[class_name]]\n",
    "                if i <= c * SPLIT_RATIO:\n",
    "                    writer.writerow(row)\n",
    "                else:\n",
    "                    writer2.writerow(row)\n",
    "\n",
    "                s += 1\n",
    "\n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "    \"\"\" preprocess_input is as good as exact mean/std\n",
    "    print(\"Calculating mean and std...\")\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    length = 0\n",
    "    images = glob.glob(\"{}/*\".format(TRAIN_FOLDER))\n",
    "    for i, path in enumerate(images):\n",
    "        print(\"{}/{}\".format(i + 1, len(images)), end=\"\\r\")\n",
    "        sum_ = np.mean(cv2.imread(path))\n",
    "        length += 1\n",
    "        mean_next = mean + (sum_ - mean) / length\n",
    "        std += (sum_ - mean) * (sum_ - mean_next)\n",
    "        mean = mean_next\n",
    "    std = np.sqrt(std / (length - 1))\n",
    "    print(\"\\nMean: {}\".format(mean))\n",
    "    print(\"Std: {}\".format(std))\n",
    "    \"\"\"\n",
    "pet_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.35, 0.5, 0.75, 1.0\n",
    "ALPHA = 1.0\n",
    "\n",
    "# 96, 128, 160, 192, 224\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 50\n",
    "\n",
    "MULTI_PROCESSING = False\n",
    "THREADS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.paths = []\n",
    "\n",
    "        with open(csv_file, \"r\") as file:\n",
    "            self.coords = np.zeros((sum(1 for line in file), 4))\n",
    "            file.seek(0)\n",
    "\n",
    "            reader = csv.reader(file, delimiter=\",\")\n",
    "            for index, row in enumerate(reader):\n",
    "                for i, r in enumerate(row[1:7]):\n",
    "                    row[i+1] = int(r)\n",
    "\n",
    "                path, image_height, image_width, x0, y0, x1, y1, _, _ = row\n",
    "                self.coords[index, 0] = x0 * IMAGE_SIZE / image_width\n",
    "                self.coords[index, 1] = y0 * IMAGE_SIZE / image_height\n",
    "                self.coords[index, 2] = (x1 - x0) * IMAGE_SIZE / image_width\n",
    "                self.coords[index, 3] = (y1 - y0) * IMAGE_SIZE / image_height \n",
    "\n",
    "                self.paths.append(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.coords) / BATCH_SIZE)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.paths[idx * BATCH_SIZE:(idx + 1) * BATCH_SIZE]\n",
    "        batch_coords = self.coords[idx * BATCH_SIZE:(idx + 1) * BATCH_SIZE]\n",
    "\n",
    "        batch_images = np.zeros((len(batch_paths), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n",
    "        for i, f in enumerate(batch_paths):\n",
    "            img = Image.open(IMAGE_FOLDER + f)\n",
    "            img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "            batch_images[i] = preprocess_input(np.array(img, dtype=np.float32))\n",
    "            img.close()\n",
    "\n",
    "        return batch_images, batch_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_coords = data_generator.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation(Callback):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        mse = 0\n",
    "        intersections = 0\n",
    "        unions = 0\n",
    "\n",
    "        for i in range(len(self.generator)):\n",
    "            batch_images, gt = self.generator[i]\n",
    "            pred = self.model.predict_on_batch(batch_images)\n",
    "            mse += np.linalg.norm(gt - pred, ord='fro') / pred.shape[0]\n",
    "\n",
    "            pred = np.maximum(pred, 0)\n",
    "\n",
    "            diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n",
    "            diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n",
    "            intersection = np.maximum(diff_width, 0) * np.maximum(diff_height, 0)\n",
    "\n",
    "            area_gt = gt[:,2] * gt[:,3]\n",
    "            area_pred = pred[:,2] * pred[:,3]\n",
    "            union = np.maximum(area_gt + area_pred - intersection, 0)\n",
    "\n",
    "            intersections += np.sum(intersection * (union > 0))\n",
    "            unions += np.sum(union)\n",
    "\n",
    "        iou = np.round(intersections / (unions + epsilon()), 4)\n",
    "        logs[\"val_iou\"] = iou\n",
    "\n",
    "        mse = np.round(mse, 4)\n",
    "        logs[\"val_mse\"] = mse\n",
    "\n",
    "        print(\" - val_iou: {} - val_mse: {}\".format(iou, mse))\n",
    "\n",
    "def create_model(trainable=False):\n",
    "    model = MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, alpha=ALPHA)\n",
    "\n",
    "    # to freeze layers\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    x = model.layers[-1].output\n",
    "    x = Conv2D(4, kernel_size=3, name=\"coords\")(x)\n",
    "    x = Reshape((4,))(x)\n",
    "\n",
    "    return Model(inputs=model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGenerator(TRAIN_CSV)\n",
    "validation_datagen = Validation(generator=DataGenerator(VALIDATION_CSV))\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model-{val_iou:.2f}.h5\", monitor=\"val_iou\", verbose=1, save_best_only=True,\n",
    "                             save_weights_only=True, mode=\"max\")\n",
    "early_stop = EarlyStopping(monitor=\"val_iou\", patience=PATIENCE, mode=\"max\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_iou\", factor=0.2, patience=10, min_lr=1e-7, verbose=1, mode=\"max\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=TENSORBOARD_LOGDIR)\n",
    "\n",
    "\n",
    "model.fit_generator(generator=train_datagen,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[validation_datagen, checkpoint, reduce_lr, early_stop, tensorboard_callback],\n",
    "                    workers=THREADS,\n",
    "                    use_multiprocessing=MULTI_PROCESSING,\n",
    "                    shuffle=True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_FILE = \"model-0.44.h5\"\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/zzhang/Documents/Workspace/Dataset/Pet/images/yorkshire_terrier_188.jpg\"\n",
    "\n",
    "unscaled = cv2.imread(filename)\n",
    "image_height, image_width, _ = unscaled.shape\n",
    "\n",
    "image = cv2.resize(unscaled, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "feat_scaled = preprocess_input(np.array(image, dtype=np.float32))\n",
    "\n",
    "region = model.predict(x=np.array([feat_scaled]))[0]\n",
    "\n",
    "x0 = int(region[0] * image_width / IMAGE_SIZE)\n",
    "y0 = int(region[1] * image_height / IMAGE_SIZE)\n",
    "\n",
    "x1 = int((region[0] + region[2]) * image_width / IMAGE_SIZE)\n",
    "y1 = int((region[1] + region[3]) * image_height / IMAGE_SIZE)\n",
    "\n",
    "cv2.rectangle(unscaled, (x0, y0), (x1, y1), (0, 0, 255), 1)\n",
    "cv2.imshow(\"image\", unscaled)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
